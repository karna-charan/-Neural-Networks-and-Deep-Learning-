{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtnQZfKivmghOkooq+7r5O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karna-charan/-Neural-Networks-and-Deep-Learning-/blob/main/handwritten.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load MNIST dataset (handwritten digits 0–9)\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values from 0–255 to 0–1\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Build the neural network model\n",
        "model = models.Sequential([\n",
        "\n",
        "    # Flatten 28x28 image into 1D vector (784)\n",
        "    layers.Flatten(input_shape=(28, 28)),\n",
        "\n",
        "    # Hidden layer with 128 neurons and ReLU activation\n",
        "    layers.Dense(128, activation='relu'),\n",
        "\n",
        "    # Output layer with 10 neurons (digits 0–9)\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Predict digit for first test image\n",
        "prediction = model.predict(X_test[:1])\n",
        "predicted_digit = prediction.argmax()\n",
        "\n",
        "print(\"Predicted Digit:\", predicted_digit)\n",
        "print(\"Actual Digit:\", y_test[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmH7burMZVQG",
        "outputId": "e102e899-bbe0-47c0-d20c-0a4a8a7c4720"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8743 - loss: 0.4496 - val_accuracy: 0.9653 - val_loss: 0.1202\n",
            "Epoch 2/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9647 - loss: 0.1229 - val_accuracy: 0.9735 - val_loss: 0.0908\n",
            "Epoch 3/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9748 - loss: 0.0834 - val_accuracy: 0.9777 - val_loss: 0.0808\n",
            "Epoch 4/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9810 - loss: 0.0626 - val_accuracy: 0.9778 - val_loss: 0.0856\n",
            "Epoch 5/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9856 - loss: 0.0458 - val_accuracy: 0.9763 - val_loss: 0.0860\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9686 - loss: 0.1020\n",
            "Test Accuracy: 0.9725000262260437\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "Predicted Digit: 7\n",
            "Actual Digit: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1npJ13KWZUot",
        "outputId": "b9075a59-a0d0-4b65-d4fe-ee9a2a766ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 50ms/step - accuracy: 0.8877 - loss: 0.3805 - val_accuracy: 0.9833 - val_loss: 0.0582\n",
            "Epoch 2/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 49ms/step - accuracy: 0.9829 - loss: 0.0576 - val_accuracy: 0.9883 - val_loss: 0.0417\n",
            "Epoch 3/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 49ms/step - accuracy: 0.9898 - loss: 0.0346 - val_accuracy: 0.9880 - val_loss: 0.0367\n",
            "Epoch 4/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 54ms/step - accuracy: 0.9920 - loss: 0.0243 - val_accuracy: 0.9910 - val_loss: 0.0323\n",
            "Epoch 5/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 48ms/step - accuracy: 0.9943 - loss: 0.0158 - val_accuracy: 0.9913 - val_loss: 0.0346\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9875 - loss: 0.0380\n",
            "Test accuracy: 0.9902999997138977\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Install packages (if not already installed)\n",
        "!pip install tensorflow matplotlib\n",
        "\n",
        "# Load dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values (0–255 → 0–1)\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Add channel dimension (required for CNN)\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')  # 10 digits (0–9)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Pick a random test image\n",
        "index = 0\n",
        "image = x_test[index]\n",
        "\n",
        "prediction = model.predict(image[np.newaxis, ...])\n",
        "predicted_digit = np.argmax(prediction)\n",
        "\n",
        "plt.imshow(image.squeeze(), cmap='gray')\n",
        "plt.title(f\"Predicted Digit: {predicted_digit}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Imports\n",
        "# =========================\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# =========================\n",
        "# Load MNIST dataset\n",
        "# =========================\n",
        "# x_train, x_test: images (28x28 grayscale)\n",
        "# y_train, y_test: digit labels (0–9)\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# =========================\n",
        "# Create double-digit images\n",
        "# =========================\n",
        "def create_double_digit(images, labels):\n",
        "    \"\"\"\n",
        "    Creates images with two digits side-by-side.\n",
        "    Returns:\n",
        "        X  -> combined images (28x56x1)\n",
        "        y1 -> left digit labels\n",
        "        y2 -> right digit labels\n",
        "    \"\"\"\n",
        "    X, y1, y2 = [], [], []\n",
        "\n",
        "    for i in range(len(images)):\n",
        "        # Randomly choose a second image\n",
        "        j = np.random.randint(0, len(images))\n",
        "\n",
        "        # Concatenate images horizontally (left + right)\n",
        "        img = np.concatenate([images[i], images[j]], axis=1)\n",
        "        X.append(img)\n",
        "\n",
        "        # Store labels for each digit\n",
        "        y1.append(labels[i])\n",
        "        y2.append(labels[j])\n",
        "\n",
        "    # Convert to NumPy array and add channel dimension\n",
        "    X = np.array(X)[..., np.newaxis]\n",
        "\n",
        "    return X, np.array(y1), np.array(y2)\n",
        "\n",
        "# Generate training and testing data\n",
        "X_train, y_left_train, y_right_train = create_double_digit(x_train, y_train)\n",
        "X_test, y_left_test, y_right_test = create_double_digit(x_test, y_test)\n",
        "\n",
        "# =========================\n",
        "# Build the CNN model\n",
        "# =========================\n",
        "inputs = layers.Input(shape=(28, 56, 1))  # Two MNIST digits side by side\n",
        "\n",
        "# First convolution block\n",
        "x = layers.Conv2D(32, 3, activation='relu')(inputs)\n",
        "x = layers.MaxPooling2D()(x)\n",
        "\n",
        "# Second convolution block\n",
        "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D()(x)\n",
        "\n",
        "# Fully connected layers\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "# Output heads (multi-output model)\n",
        "left = layers.Dense(10, activation='softmax', name='left_digit')(x)\n",
        "right = layers.Dense(10, activation='softmax', name='right_digit')(x)\n",
        "\n",
        "# Define the model\n",
        "model = models.Model(inputs, [left, right])\n",
        "\n",
        "# =========================\n",
        "# Compile the model\n",
        "# =========================\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss={\n",
        "        'left_digit': 'sparse_categorical_crossentropy',\n",
        "        'right_digit': 'sparse_categorical_crossentropy'\n",
        "    },\n",
        "    metrics={\n",
        "        'left_digit': 'accuracy',\n",
        "        'right_digit': 'accuracy'\n",
        "    }\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# Train the model\n",
        "# =========================\n",
        "model.fit(\n",
        "    X_train,\n",
        "    {\n",
        "        'left_digit': y_left_train,\n",
        "        'right_digit': y_right_train\n",
        "    },\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# Test on one image\n",
        "# =========================\n",
        "idx = 0  # Change index to test different samples\n",
        "image = X_test[idx:idx+1]\n",
        "\n",
        "# Predict both digits\n",
        "pred_left, pred_right = model.predict(image)\n",
        "\n",
        "# Convert probabilities to digit predictions\n",
        "d1 = np.argmax(pred_left)\n",
        "d2 = np.argmax(pred_right)\n",
        "\n",
        "# Combine digits into a two-digit number\n",
        "predicted_digit = d1 * 10 + d2\n",
        "\n",
        "# =========================\n",
        "# Display result\n",
        "# =========================\n",
        "plt.imshow(image[0].squeeze(), cmap='gray')\n",
        "plt.title(f\"Predicted Digit: {predicted_digit}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "EMe6n2RW5EOJ",
        "outputId": "8b0d0a09-abd6-4635-bb2f-54869b0a41be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 107ms/step - left_digit_accuracy: 0.8652 - left_digit_loss: 0.4338 - loss: 0.8655 - right_digit_accuracy: 0.8618 - right_digit_loss: 0.4317 - val_left_digit_accuracy: 0.9778 - val_left_digit_loss: 0.0716 - val_loss: 0.1315 - val_right_digit_accuracy: 0.9828 - val_right_digit_loss: 0.0597\n",
            "Epoch 2/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 102ms/step - left_digit_accuracy: 0.9826 - left_digit_loss: 0.0606 - loss: 0.1140 - right_digit_accuracy: 0.9827 - right_digit_loss: 0.0533 - val_left_digit_accuracy: 0.9820 - val_left_digit_loss: 0.0617 - val_loss: 0.1267 - val_right_digit_accuracy: 0.9808 - val_right_digit_loss: 0.0647\n",
            "Epoch 3/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 102ms/step - left_digit_accuracy: 0.9884 - left_digit_loss: 0.0376 - loss: 0.0688 - right_digit_accuracy: 0.9895 - right_digit_loss: 0.0312 - val_left_digit_accuracy: 0.9858 - val_left_digit_loss: 0.0458 - val_loss: 0.0834 - val_right_digit_accuracy: 0.9885 - val_right_digit_loss: 0.0374\n",
            "Epoch 4/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 103ms/step - left_digit_accuracy: 0.9925 - left_digit_loss: 0.0235 - loss: 0.0408 - right_digit_accuracy: 0.9947 - right_digit_loss: 0.0173 - val_left_digit_accuracy: 0.9863 - val_left_digit_loss: 0.0499 - val_loss: 0.0951 - val_right_digit_accuracy: 0.9878 - val_right_digit_loss: 0.0453\n",
            "Epoch 5/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 104ms/step - left_digit_accuracy: 0.9939 - left_digit_loss: 0.0189 - loss: 0.0316 - right_digit_accuracy: 0.9959 - right_digit_loss: 0.0127 - val_left_digit_accuracy: 0.9893 - val_left_digit_loss: 0.0429 - val_loss: 0.0790 - val_right_digit_accuracy: 0.9900 - val_right_digit_loss: 0.0360\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CQ27lQnG5E3g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}